{"cells":[{"cell_type":"markdown","id":"BYYL_jnG0B3i","metadata":{"id":"BYYL_jnG0B3i"},"source":["# <font color = 'indianred'> **Logistic Regression using Minibatch Stochastic Gradient Descent with PyTorch**"]},{"cell_type":"markdown","id":"o6jh4FfFPgi_","metadata":{"id":"o6jh4FfFPgi_"},"source":["## <font color = 'indianred'>**Data**"]},{"cell_type":"code","execution_count":null,"id":"1pN2psBZPquN","metadata":{"execution":{"iopub.execute_input":"2022-10-26T19:14:42.405079Z","iopub.status.busy":"2022-10-26T19:14:42.404795Z","iopub.status.idle":"2022-10-26T19:14:42.725729Z","shell.execute_reply":"2022-10-26T19:14:42.725116Z","shell.execute_reply.started":"2022-10-26T19:14:42.405000Z"},"id":"1pN2psBZPquN"},"outputs":[],"source":["from sklearn.datasets import make_moons\n","from sklearn.preprocessing import StandardScaler"]},{"cell_type":"code","execution_count":null,"id":"dHjBoauBQ809","metadata":{"id":"dHjBoauBQ809"},"outputs":[],"source":["import torch\n","import torch.optim as optim\n","import torch.nn as nn\n","import torch.functional as F\n","from torch.utils.data import DataLoader, TensorDataset"]},{"cell_type":"code","execution_count":null,"id":"WUcQ0mpp3HNm","metadata":{"id":"WUcQ0mpp3HNm"},"outputs":[],"source":["X, y = make_moons(n_samples=1000, noise=0.05, random_state=0)"]},{"cell_type":"code","execution_count":null,"id":"rANPOz2gM6jM","metadata":{"id":"rANPOz2gM6jM"},"outputs":[],"source":["preprocessor = StandardScaler()\n","X = preprocessor.fit_transform(X)"]},{"cell_type":"code","execution_count":null,"id":"IKQM7PcKVDkK","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1693203606740,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"IKQM7PcKVDkK","outputId":"f0871745-ac1b-4212-90cb-b9d5e35a3a9e"},"outputs":[{"output_type":"stream","name":"stdout","text":["(1000, 2) (1000,)\n"]}],"source":["print(X.shape, y.shape)"]},{"cell_type":"code","execution_count":null,"id":"YhLylG90VHG5","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1693203607837,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"YhLylG90VHG5","outputId":"752b5497-bc3a-4833-83d4-2bba8187bf9f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 1.75081891,  0.48393978],\n","       [ 1.35606443, -0.90706929],\n","       [-0.90150442,  1.22470664],\n","       [-0.60117222, -0.14688373],\n","       [ 0.0048725 , -1.28700543]])"]},"metadata":{},"execution_count":6}],"source":["X[0:5]"]},{"cell_type":"code","execution_count":null,"id":"a8ZlJ168VLQL","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":151,"status":"ok","timestamp":1693203610970,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"a8ZlJ168VLQL","outputId":"6efbb7fd-0e8e-4b92-cb4c-a96f63d85616"},"outputs":[{"output_type":"stream","name":"stdout","text":["[1 1 0 1 1 1 0 0 0 1]\n"]}],"source":["print(y[0:10])"]},{"cell_type":"markdown","id":"tUYUlTaEQW2w","metadata":{"id":"tUYUlTaEQW2w"},"source":["## <font color = 'indianred'>**Dataset and Data Loaders**"]},{"cell_type":"code","execution_count":null,"id":"hqQS3ENqNE2l","metadata":{"id":"hqQS3ENqNE2l"},"outputs":[],"source":["# Create Tensors from numpy\n","x_tensor = torch.tensor(X).float()\n","y_tensor = torch.tensor(y.reshape(-1, 1)).float()\n","\n","# create a PyTorch Dataset from x and y tensors\n","train_dataset = TensorDataset(x_tensor, y_tensor)\n","\n","# CReate Data lOader from Dataset\n","train_loader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True)"]},{"cell_type":"markdown","id":"189Bp-1CRTWb","metadata":{"id":"189Bp-1CRTWb"},"source":["## <font color = 'indianred'>**Loss function**"]},{"cell_type":"markdown","source":["**Binary Classification - using Sigmoid**\n","\n","<img src =\"https://drive.google.com/uc?export=view&id=1DT7n0Lmbt1hzUvSH9AS9SSydEvOJEPbE\" >"],"metadata":{"id":"hmetzbpH1Vij"},"id":"hmetzbpH1Vij"},{"cell_type":"markdown","source":["### <font color = 'indianred'>**nn.BCELoss()**"],"metadata":{"id":"aDu7Tr3tsdac"},"id":"aDu7Tr3tsdac"},{"cell_type":"code","source":["torch.manual_seed(42)\n","input = torch.randn(6, requires_grad=True).view(3,2)\n","print(f'input.shape:{input.shape}\\n')\n","target = torch.tensor([1.0, 0., 1.0]).view(3,1)\n","print(f'target.shape:{input.shape}\\n')\n","\n","# model - with sigmoid activation for outpur layer\n","model = # code here\n","output = model(input)\n","print(f'output:{output}\\n')\n","\n","# BCE Loss function\n","loss_fn = # code here\n","loss = loss_fn(output, target)\n","print(f'loss:{loss}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YNbzXxyUi6Yu","executionInfo":{"status":"ok","timestamp":1693203619337,"user_tz":300,"elapsed":175,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"c89cb0f4-a34b-4a74-8d2a-d24572dc7a8a"},"id":"YNbzXxyUi6Yu","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["input.shape:torch.Size([3, 2])\n","\n","target.shape:torch.Size([3, 2])\n","\n","output:tensor([[0.6293],\n","        [0.6190],\n","        [0.4345]], grad_fn=<SigmoidBackward0>)\n","\n","loss:0.7539259791374207\n"]}]},{"cell_type":"markdown","source":["### <font color = 'indianred'>**nn.BCEWithLogitsLoss()**"],"metadata":{"id":"e_CFDLsBsrOt"},"id":"e_CFDLsBsrOt"},{"cell_type":"code","source":["torch.manual_seed(42)\n","input = torch.randn(6, requires_grad=True).view(3,2)\n","print(f'input.shape:{input.shape}\\n')\n","target = torch.tensor([1.0, 0., 1.0]).view(3,1)\n","print(f'target.shape:{input.shape}\\n')\n","\n","# model - without sigmoid activation for outpur layer\n","model = # code here\n","output = model(input)\n","print(f'output:{output}\\n')\n","\n","# BCE with Logistic Loss function\n","loss_fn = # code here\n","loss = loss_fn(output, target)\n","print(f'loss:{loss}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q7VICYTZjfTp","executionInfo":{"status":"ok","timestamp":1693203787718,"user_tz":300,"elapsed":109,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"08dacd42-e6ff-4742-c97a-824a4b697b14"},"id":"Q7VICYTZjfTp","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["input.shape:torch.Size([3, 2])\n","\n","target.shape:torch.Size([3, 2])\n","\n","output:tensor([[ 0.5292],\n","        [ 0.4855],\n","        [-0.2635]], grad_fn=<AddmmBackward0>)\n","\n","loss:0.7539259791374207\n"]}]},{"cell_type":"markdown","source":["<font color= 'indianred' fomnt size = 5>**Both `nn.BCELoss()` and `nn.BCEWithLogitsLoss()` are used for binary classification problems** </font>, but they differ in how they accept input and perform calculations, impacting their numerical stability and performance.\n","\n","### nn.BCELoss():\n","\n","1. **Input Range**: Expects the input tensor to be in the range \\([0, 1]\\), typically the output of a sigmoid activation function.\n","2. **Target**: Binary labels that are either 0 or 1.\n","3. **Standalone**: Applies only the Binary Cross Entropy loss function.\n","\n","### nn.BCEWithLogitsLoss():\n","\n","1. **Input Range**: Accepts raw scores (logits) without any activation function applied.\n","2. **Target**: Binary labels that are either 0 or 1.\n","3. **Combined**: Applies both the sigmoid activation function and the Binary Cross Entropy loss in a single, more numerically stable step.\n","\n","#### Why nn.BCEWithLogitsLoss() is Generally Preferred:\n","\n","1. **Numerical Stability**: Using `nn.BCEWithLogitsLoss()` is more numerically stable than using `nn.BCELoss()` with a separate sigmoid activation. This is because `nn.BCEWithLogitsLoss()` combines the sigmoid activation and the loss calculation into a single operation, which can avoid some of the numerical instability incurred by calculating them separately.\n","\n","2. **Performance**: Combining the sigmoid operation with the loss calculation can lead to optimizations. Backpropagation through a combined operation is often faster than through separate operations.\n","\n","3. **Memory Efficiency**: Since you don't need to store the intermediate sigmoid outputs for backpropagation, using `nn.BCEWithLogitsLoss()` can be more memory-efficient, particularly important for large-scale models.\n","\n","So, for better numerical stability, performance, and memory efficiency, `nn.BCEWithLogitsLoss()` is generally the recommended choice."],"metadata":{"id":"W0q0YTWDz232"},"id":"W0q0YTWDz232"},{"cell_type":"markdown","source":["<br><br><br><br>\n","\n","**Binary Classification - using Softmax**\n","\n","<img src =\"https://drive.google.com/uc?export=view&id=1DVSgL5tEvWRYt4XhlqwEEAhu8FOqt9wK\" >\n","\n"],"metadata":{"id":"_hJO0-tY2F05"},"id":"_hJO0-tY2F05"},{"cell_type":"markdown","source":["### <font color = 'indianred'>**nn.NLLLoss()**"],"metadata":{"id":"uqToyS6LuClY"},"id":"uqToyS6LuClY"},{"cell_type":"code","source":["torch.manual_seed(42)\n","input = torch.randn(6, requires_grad=True).view(3,2)\n","print(f'input.shape:{input.shape}\\n')\n","target = torch.tensor([1.0, 0., 1.0]).long()\n","print(f'target.shape:{input.shape}\\n')\n","\n","# model - without sigmoid activation for outpur layer\n","model = # code here\n","output = model(input)\n","print(f'output:{output}\\n')\n","\n","# Negative Log Likelihood Function\n","loss_fn = # code here\n","loss = loss_fn(output, target)\n","print(f'loss:{loss}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-7UaZ45Ep2cT","executionInfo":{"status":"ok","timestamp":1693203628014,"user_tz":300,"elapsed":165,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"9d4c9219-47af-4245-ec21-d44522951ecd"},"id":"-7UaZ45Ep2cT","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["input.shape:torch.Size([3, 2])\n","\n","target.shape:torch.Size([3, 2])\n","\n","output:tensor([[-0.4640, -0.9909],\n","        [-0.4635, -0.9917],\n","        [-0.5980, -0.7984]], grad_fn=<LogSoftmaxBackward0>)\n","\n","loss:0.7509231567382812\n"]}]},{"cell_type":"markdown","source":["### <font color = 'indianred'>**nn.CrossEntropy()**"],"metadata":{"id":"LpMT910VuNvG"},"id":"LpMT910VuNvG"},{"cell_type":"code","source":["torch.manual_seed(42)\n","input = torch.randn(6, requires_grad=True).view(3,2)\n","print(f'input.shape:{input.shape}\\n')\n","target = torch.tensor([1.0, 0., 1.0]).long()\n","print(f'target.shape:{input.shape}\\n')\n","\n","# model - without sigmoid activation for outpur layer\n","model = # code here\n","output = model(input)\n","print(f'output:{output}\\n')\n","\n","# CrossEntropy Function\n","loss_fn = # code here\n","loss = loss_fn(output, target)\n","print(f'loss:{loss}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YE8XXy2-mzLN","executionInfo":{"status":"ok","timestamp":1693204002840,"user_tz":300,"elapsed":117,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"700a8f79-9e87-44d9-c3f0-823ca6afab74"},"id":"YE8XXy2-mzLN","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["input.shape:torch.Size([3, 2])\n","\n","target.shape:torch.Size([3, 2])\n","\n","output:tensor([[ 0.7333,  0.2065],\n","        [ 0.6896,  0.1615],\n","        [-0.0593, -0.2597]], grad_fn=<AddmmBackward0>)\n","\n","loss:0.7509231567382812\n"]}]},{"cell_type":"markdown","source":["<font color= 'indianred' fomnt size = 5>**Both `nn.NLLLoss()` and `nn.CrossEntropyLoss()` are used for classification tasks** </font>, but they operate on different kinds of inputs and perform different computations. Here's a breakdown:\n","\n","### nn.NLLLoss():\n","\n","1. **Input**: Expects log probabilities, typically the output of a `log_softmax` function.\n","2. **Target**: Requires class labels as integers.\n","3. **Operation**: Applies the Negative Log Likelihood (NLL) loss, essentially indexing into the log probabilities based on target labels and negating the values.\n","\n","### nn.CrossEntropyLoss():\n","\n","1. **Input**: Expects raw scores (logits), without any activation function applied.\n","2. **Target**: Requires class labels as integers.\n","3. **Operation**: Combines both the `log_softmax` and `nn.NLLLoss()` in a single operation, making it a more convenient and numerically stable option.\n","\n","#### Why `nn.CrossEntropyLoss()` is Generally Preferred:\n","\n","1. **Numerical Stability**: Similar to the advantage of `nn.BCEWithLogitsLoss()` over `nn.BCELoss()`, `nn.CrossEntropyLoss()` improves numerical stability by combining `log_softmax` and `nn.NLLLoss()` into a single operation.\n","\n","2. **Code Simplification**: You don't have to explicitly include a softmax or log_softmax layer before the loss layer, making the code simpler and less prone to errors.\n","\n","3. **Performance**: The combined operation can result in a performance gain during both forward and backward passes, as some computations can be merged.\n","\n","4. **Memory Efficiency**: No need to store intermediate values from the softmax operation when using `nn.CrossEntropyLoss()`, potentially saving memory.\n","\n","In summary, `nn.CrossEntropyLoss()` is often the more convenient and numerically stable option for classification tasks. It encapsulates the functionalities of `log_softmax` and `nn.NLLLoss()` in a single class, offering advantages in terms of code clarity, performance, and memory efficiency."],"metadata":{"id":"0_YFtoAz0rZQ"},"id":"0_YFtoAz0rZQ"},{"cell_type":"markdown","source":["### <font color = 'indianred'>**Summary Loss Functions**"],"metadata":{"id":"XGTcPVobuxWC"},"id":"XGTcPVobuxWC"},{"cell_type":"markdown","source":["<font size = 10></font>\n","\n","|                              | <font size = 4> BCE Loss| <font size = 4> BCE With Logits Loss| <font size = 4> NLL Loss| <font size = 4> Cross Entropy Loss|\n","|--------------------------|-------------|-----------|---------|-----------|\n","|<font size = 4> Classification|<font size = 4> binary| <font size = 4> binary| <font size = 4> multiclass|<font size = 4> multiclass|\n","|<font size = 4> Input (each datapoint)| <font size = 4> probbaility|<font size = 4> logit|<font size = 4> array of log probbailities|<font size = 4> array of logits|\n","<font size = 4> Label (Each data point)| <font size = 4> float (0.0 or 1.0)|<font size = 4> float (0.0 or 1.0)|<font size = 4> long(class index)|<font size = 4> long(class index)|\n","|<font size = 4> Model's Last layer| <font size = 4> Sigmoid | - | <font size = 4> LogSoftmax|-"],"metadata":{"id":"IUQL8DEDu5c8"},"id":"IUQL8DEDu5c8"},{"cell_type":"markdown","source":["### <font color = 'indianred'>**Recommendation**\n","- **nn.BCEWithLogitsLoss() for binary classification**\n","- **nn.CrossEntropyLoss() for multi-class classification**\n","\n"],"metadata":{"id":"kiWKeaBdx9V_"},"id":"kiWKeaBdx9V_"},{"cell_type":"markdown","metadata":{"id":"y3HFDKy1RLMj"},"source":["## <font color = 'indianred'>**Model**"],"id":"y3HFDKy1RLMj"},{"cell_type":"code","execution_count":null,"metadata":{"id":"mtFhXfXpROP3"},"outputs":[],"source":["# Speciy your model\n","model = # code here"],"id":"mtFhXfXpROP3"},{"cell_type":"code","execution_count":null,"id":"ZyxG9cEIRdxD","metadata":{"id":"ZyxG9cEIRdxD"},"outputs":[],"source":["# specify your loss function\n","loss_function = # code here"]},{"cell_type":"markdown","id":"Mu2DOkq5SH_X","metadata":{"id":"Mu2DOkq5SH_X"},"source":["## <font color = 'indianred'>**Optimizer**"]},{"cell_type":"code","execution_count":null,"id":"z2ZPyfdlSMi9","metadata":{"id":"z2ZPyfdlSMi9"},"outputs":[],"source":["# specify optimizer\n","learning_rate = 0.1\n","optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate )"]},{"cell_type":"markdown","id":"Z0ETm1C0apXS","metadata":{"id":"Z0ETm1C0apXS"},"source":["## <font color = 'indianred'>**Initialization**\n","\n","Create a function to initilaize weights.\n","- Initialize weights using normal distribution with mean = 0 and std = 0.05\n","- Initilaize the bias term with zeros"]},{"cell_type":"code","execution_count":null,"id":"P7_uk-Wya5w-","metadata":{"id":"P7_uk-Wya5w-"},"outputs":[],"source":["def init_weights(layer):\n","  if type(layer) == nn.Linear:\n","    torch.nn.init.normal_(layer.weight, mean = 0, std = 0.05)\n","    torch.nn.init.zeros_(layer.bias)"]},{"cell_type":"markdown","id":"_s37DSbzSkWF","metadata":{"id":"_s37DSbzSkWF"},"source":["## <font color = 'indianred'>**Training Loop**"]},{"cell_type":"markdown","id":"0m8CNyv7Syxx","metadata":{"id":"0m8CNyv7Syxx"},"source":["**Model Training** involves five steps:\n","\n","- Step 0: Randomly initialize parameters / weights\n","- Step 1: Compute model's predictions - forward pass\n","- Step 2: Compute loss\n","- Step 3: Compute the gradients\n","- Step 4: Update the parameters\n","- Step 5: Repeat steps 1 - 4\n","\n","Model training is repeating this process over and over, for many **epochs**.\n","\n","We will specify number of ***epochs*** and during each epoch we will iterate over the complete dataset and will keep on updating the parameters.\n","\n","***Learning rate*** and ***epochs*** are known as hyperparameters. We have to adjust the values of these two based on validation dataset.\n","\n","We will now create functions for step 1 to 4."]},{"cell_type":"code","execution_count":null,"id":"zRFZkBl-TGPC","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3028,"status":"ok","timestamp":1693203654211,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"zRFZkBl-TGPC","outputId":"35dafba2-16a3-4e25-954d-0c43930c0d58"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda:0\n","Epoch : 1 / 5\n","Train Loss:  0.4049 | Train Accuracy:  86.3000%\n","Epoch : 2 / 5\n","Train Loss:  0.2908 | Train Accuracy:  86.6000%\n","Epoch : 3 / 5\n","Train Loss:  0.2648 | Train Accuracy:  87.0000%\n","Epoch : 4 / 5\n","Train Loss:  0.2562 | Train Accuracy:  87.5000%\n","Epoch : 5 / 5\n","Train Loss:  0.2502 | Train Accuracy:  87.8000%\n"]}],"source":["torch.manual_seed(100)\n","epochs = 5\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print(device)\n","\n","# move model tp gpu\n","# code here\n","\n","# Step 0: Randomly initialize parameters / weights\n","\n","model.apply(init_weights)\n","\n","for epoch in range(epochs):\n","\n","  # Training Loop\n","  # Initialize train_loss at the he start of the epoch\n","  running_train_loss = 0\n","  running_train_correct = 0\n","\n","  # Iterate on batches from the dataset using train_loader\n","  for input_, targets in train_loader:\n","\n","    # move inputs and outputs to GPUs\n","    input_ = # code here\n","    targets = # code here\n","\n","    # Step 1: Forward Pass: Compute model's predictions\n","    output = model(input_)\n","\n","    # Step 2: Compute loss\n","    loss = loss_function(output, targets)\n","\n","    # Correct prediction\n","    y_pred = # code here # get the predicted class\n","    correct = # code here\n","\n","    # Step 3: Backward pass -Compute the gradients\n","    optimizer.zero_grad()\n","    loss.backward()\n","\n","    # Step 4: Update the parameters\n","    optimizer.step()\n","\n","    # Add train loss of a batch\n","    running_train_loss += loss.item()\n","\n","    # Add Corect counts of a batch\n","    running_train_correct += correct\n","\n","  # Calculate mean train loss for the whole dataset for a particular epoch\n","  train_loss = running_train_loss/len(train_loader)\n","\n","  # Calculate accuracy for the whole dataset for a particular epoch\n","  train_acc = running_train_correct/len(train_loader.dataset)\n","\n","  # Print the train loss and accuracy for given number of epochs, batch size and number of samples\n","  print(f'Epoch : {epoch+1} / {epochs}')\n","  print(f'Train Loss: {train_loss : .4f} | Train Accuracy: {train_acc * 100 : .4f}%')"]},{"cell_type":"code","execution_count":null,"id":"6V3o_7WCYWac","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":209,"status":"ok","timestamp":1693203658066,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"6V3o_7WCYWac","outputId":"5f1b43ca-ab4e-484c-e872-ae178c8f4abf"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.weight tensor([[ 0.9944, -2.3283]], device='cuda:0')\n","0.bias tensor([-0.0195], device='cuda:0')\n"]}],"source":["# print the estimated weight and bias term\n","for name, param in model.named_parameters():\n","  print(name, param.data)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":5}